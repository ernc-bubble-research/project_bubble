# Story 3.1-4: LLM Provider Credential Storage

Status: done

## Story

**As a** Bubble Admin,
**I want** to securely store and manage API keys and provider-specific configuration for each LLM provider,
**So that** the execution engine (Epic 4) can authenticate with LLM providers without relying on environment variables.

## Background

Currently, LLM provider credentials are stored as environment variables (`GEMINI_API_KEY` in `.env`). This works for single-provider setups but doesn't scale to multiple providers or runtime configuration. This story adds a database-backed credential storage system with encryption.

### Codebase Context (Verified)

**Existing env vars** — `.env.example:37-44`: Only `GEMINI_API_KEY` and `EMBEDDING_PROVIDER`/`EMBEDDING_MODEL` exist. No `LLM_PROVIDER`, `GOOGLE_AI_STUDIO_API_KEY`, or `VERTEX_PROJECT_ID` (these were invented by the original failed story).

**Existing LLM model entity** — `libs/db-layer/src/lib/entities/llm-model.entity.ts`: Models reference `providerKey` (e.g. "google-ai-studio") but there is no corresponding provider config entity that stores credentials for that provider.

**ConfigModule** — `apps/api-gateway/src/app/app.module.ts`: Uses NestJS `ConfigModule.forRoot({ isGlobal: true })` with `.env` file. The existing `GEMINI_API_KEY` env var should become the fallback when no DB config exists.

**Entity registration** — `libs/db-layer/src/lib/entities/index.ts`: Exports 11 entities. New entity must be added here.

**Encryption** — No encryption utilities exist in the codebase currently. Need to implement using Node.js `crypto` module (AES-256-GCM).

**Provider types** — LlmModelEntity.providerKey values: "google-ai-studio", "vertex", "openai", "mock". Each provider type needs different credential fields.

### Provider Credential Schema

| Provider Key | Required Fields | Optional Fields |
|-------------|----------------|-----------------|
| `google-ai-studio` | `apiKey` | — |
| `vertex` | `projectId`, `location` | `serviceAccountJson` |
| `openai` | `apiKey` | `organizationId` |
| `mock` | — | — |

## Acceptance Criteria

1. **Provider config entity** — `LlmProviderConfigEntity` stores provider type, display name, encrypted credentials (JSON), and active status
2. **Encrypted storage** — API keys and credentials are encrypted at rest using AES-256-GCM with an application-level encryption key from env var
3. **Masked responses** — GET endpoints never return full credentials; API keys show only last 4 characters (e.g. "****abcd")
4. **CRUD API** — Admin endpoints for list, create, update provider configs
5. **Provider-specific validation** — Creating a google-ai-studio provider requires apiKey; vertex requires projectId + location
6. **UI section** — Provider configuration section in Settings page (separate from LLM Models tab, or as a sub-section within it)
7. **Env var fallback** — If no DB provider config exists for a provider key, fall back to env var (GEMINI_API_KEY → google-ai-studio)
8. **Graceful encryption key handling** — New `SETTINGS_ENCRYPTION_KEY` env var. On startup: log prominent warning if missing (do NOT crash). On credential write (POST/PATCH with credentials): return 400 with clear message "Cannot store credentials: SETTINGS_ENCRYPTION_KEY is not configured." On credential read (getDecryptedCredentials): fall back to env vars, log warning if DB credentials exist but can't be decrypted. GET /llm-providers always works (returns masked data, no decryption needed).
9. **ProviderKey validation** — Application-level validation: when creating/updating an LlmModel, validate that providerKey matches either an existing LlmProviderConfig or a known provider type enum (google-ai-studio, vertex, openai, mock). Rationale: providerKey is a type discriminator, not a FK reference — provider configs are optional (env var fallback) and the boot sequence seeds models before configs. Application validation catches typos without coupling the entities.

## Tasks

### Task 1: Create Encryption Utility
- [x] Create `apps/api-gateway/src/app/common/crypto.util.ts`
- [x] `encrypt(plaintext: string, key: string): string` — AES-256-GCM, returns `iv:authTag:ciphertext` as base64
- [x] `decrypt(encrypted: string, key: string): string` — Reverses encryption
- [x] `maskSecret(value: string, visibleChars = 4): string` — Returns `****abcd`
- [x] Unit tests for encrypt/decrypt round-trip, mask function

### Task 2: Create Provider Config Entity & DTOs
- [x] Create `libs/db-layer/src/lib/entities/llm-provider-config.entity.ts`:
  - `id` (uuid), `providerKey` (unique, max 50), `displayName` (max 100), `encryptedCredentials` (text — encrypted JSON), `isActive` (boolean, default true), `createdAt`, `updatedAt`
  - System-wide table (no tenant_id, exempt from RLS like llm_models)
- [x] Export from `libs/db-layer/src/lib/entities/index.ts`
- [x] Create DTOs in `libs/shared/src/lib/dtos/settings/`:
  - `CreateLlmProviderConfigDto`: providerKey, displayName, credentials (JSON object with provider-specific fields)
  - `UpdateLlmProviderConfigDto`: Optional displayName, credentials, isActive
  - `LlmProviderConfigResponseDto`: id, providerKey, displayName, maskedCredentials (credentials with values masked), isActive, createdAt, updatedAt
- [x] Export DTOs from shared index

### Task 3: Create Provider Config Service
- [x] Create `apps/api-gateway/src/app/settings/llm-provider-config.service.ts`
- [x] `findAll()` — Returns all configs with masked credentials
- [x] `create(dto)` — Validates provider-specific required fields, encrypts credentials, saves
- [x] `update(id, dto)` — Updates fields, re-encrypts if credentials provided
- [x] `getDecryptedCredentials(providerKey)` — Internal method for execution engine (Epic 4), returns decrypted credentials; falls back to env vars if no DB config
- [x] Inject ConfigService for `SETTINGS_ENCRYPTION_KEY`

### Task 4: Create Provider Config Controller
- [x] Create `apps/api-gateway/src/app/settings/settings.module.ts` with TypeORM.forFeature([LlmProviderConfigEntity])
- [x] Create `apps/api-gateway/src/app/settings/llm-provider-config.controller.ts`:
  - `GET /api/admin/settings/llm-providers` — List all (masked)
  - `POST /api/admin/settings/llm-providers` — Create
  - `PATCH /api/admin/settings/llm-providers/:id` — Update
- [x] Roles: BUBBLE_ADMIN only
- [x] Swagger decorators with @ApiResponse for all status codes
- [x] Register SettingsModule in AppModule imports

### Task 5: Frontend Provider Config UI
- [x] Create `apps/web/src/app/admin/settings/provider-config-list.component.ts`
- [x] Provider cards: show provider name, type, status badge, masked API key
- [x] Add/edit provider dialog with provider-type-specific form fields (google-ai-studio: apiKey; vertex: projectId, location, serviceAccountJson; openai: apiKey, organizationId; mock: no fields)
- [x] Create frontend service: `apps/web/src/app/core/services/llm-provider-config.service.ts`
- [x] Add dedicated "Providers" tab to Settings page (3 tabs total: Providers, LLM Models, System)

### Task 6: Unit Tests
- [x] Crypto utility: encrypt/decrypt round-trip, mask function
- [x] LlmProviderConfigService: CRUD operations, encryption verification, env var fallback
- [x] LlmProviderConfigController: endpoint tests with auth/role guards
- [x] Frontend components: list rendering, form validation, masked display

### Task 7: Environment Setup
- [x] Add `SETTINGS_ENCRYPTION_KEY` to `.env.example` with generation instructions (e.g. `openssl rand -base64 32`)
- [x] Add startup warning in SettingsModule constructor: log prominent `⚠️ SETTINGS_ENCRYPTION_KEY not configured` warning if missing — app continues running
- [x] Guard credential write operations (create/update with credentials): return 400 if encryption key not configured
- [x] Guard getDecryptedCredentials(): fall back to env vars if encryption key missing, log warning if DB credentials exist but can't be decrypted

### Task 8: Update LLM Model Seeding (Remove Deprecated, Add Current)
- [x] Update `RlsSetupService.seedLlmModels()` to remove deprecated Gemini 2.0 models (shutting down March 31, 2026)
- [x] Add current Gemini models:
  - `google-ai-studio` / `gemini-3-flash-preview` / Gemini 3 Flash
  - `google-ai-studio` / `gemini-3-pro-preview` / Gemini 3 Pro
  - `google-ai-studio` / `gemini-2.5-flash` / Gemini 2.5 Flash
  - `google-ai-studio` / `gemini-2.5-pro` / Gemini 2.5 Pro
  - `vertex` / `gemini-3-flash-preview` / Gemini 3 Flash (Vertex)
  - `vertex` / `gemini-3-pro-preview` / Gemini 3 Pro (Vertex)
  - `vertex` / `gemini-2.5-flash` / Gemini 2.5 Flash (Vertex)
  - `vertex` / `gemini-2.5-pro` / Gemini 2.5 Pro (Vertex)
- [x] Add OpenAI models:
  - `openai` / `gpt-5.2` / GPT-5.2
  - `openai` / `gpt-4.1` / GPT-4.1
- [x] Keep mock model: `mock` / `mock-model` / Mock LLM (Testing)
- [x] Seed default provider configs (no credentials) for each provider type: google-ai-studio, vertex, openai, mock — seeded BEFORE models
- [x] Update frontend `PROVIDER_DISPLAY_NAMES` map to include `'openai': 'OpenAI'`

## AC-to-Test Traceability

| AC | Description | Test IDs | Count |
|----|-------------|----------|-------|
| AC 1 | Provider config entity | UNIT-020, 021, 065 | 3 |
| AC 2 | Encrypted storage (AES-256-GCM) | UNIT-001–010, 021, 029, 032 | 13 |
| AC 3 | Masked responses | UNIT-011–016, 018, 019, 036, 037, 051 | 11 |
| AC 4 | CRUD API | UNIT-017, 020–022, 028–030, 038–040 | 10 |
| AC 5 | Provider-specific validation | UNIT-023, 024, 026, 027, 070 | 5 |
| AC 6 | UI section (Providers tab) | UNIT-041–063 | 23 |
| AC 7 | Env var fallback | UNIT-033–035 | 3 |
| AC 8 | Graceful encryption key handling | UNIT-025, 031, 034, 037 | 4 |
| AC 9 | ProviderKey validation | UNIT-023, 064 | 2 |
| Task 8 | Model & provider config seeding | UNIT-065–069 | 5 |

**Test ID prefix**: `3.1-4-` (e.g., UNIT-001 = `[3.1-4-UNIT-001]`)

**Total**: 70 tests across 7 spec files (16 crypto, 22 service, 3 controller, 12 list, 11 dialog, 5 seeding, 1 model-service)

## Files to Create/Modify

| File | Action | Details |
|------|--------|---------|
| `apps/api-gateway/src/app/common/crypto.util.ts` | CREATE | AES-256-GCM encrypt/decrypt/mask |
| `apps/api-gateway/src/app/common/crypto.util.spec.ts` | CREATE | Encryption tests |
| `libs/db-layer/src/lib/entities/llm-provider-config.entity.ts` | CREATE | Provider config entity |
| `libs/db-layer/src/lib/entities/index.ts` | MODIFY | Export new entity |
| `libs/shared/src/lib/dtos/settings/` | CREATE | DTOs directory + 3 DTO files |
| `libs/shared/src/lib/dtos/index.ts` | MODIFY | Export new DTOs |
| `apps/api-gateway/src/app/settings/settings.module.ts` | CREATE | NestJS module |
| `apps/api-gateway/src/app/settings/llm-provider-config.service.ts` | CREATE | Service with encryption |
| `apps/api-gateway/src/app/settings/llm-provider-config.service.spec.ts` | CREATE | Service tests |
| `apps/api-gateway/src/app/settings/llm-provider-config.controller.ts` | CREATE | Admin controller |
| `apps/api-gateway/src/app/settings/llm-provider-config.controller.spec.ts` | CREATE | Controller tests |
| `apps/api-gateway/src/app/app.module.ts` | MODIFY | Import SettingsModule |
| `apps/web/src/app/core/services/llm-provider-config.service.ts` | CREATE | Frontend provider config service |
| `apps/web/src/app/admin/settings/provider-config-list.component.ts` | CREATE | Provider list UI |
| `apps/web/src/app/admin/settings/provider-config-form-dialog.component.ts` | CREATE | Add/edit dialog |
| `libs/db-layer/src/lib/rls-setup.service.ts` | MODIFY | Update seedLlmModels() — remove deprecated 2.0, add 2.5 + 3.0 + OpenAI models, seed default provider configs |
| `apps/web/src/app/admin/settings/settings.component.ts` | MODIFY | Add "Providers" tab (3 tabs: Providers, LLM Models, System) |
| `apps/web/src/app/admin/settings/llm-models-list.component.ts` | MODIFY | Import PROVIDER_DISPLAY_NAMES from shared constants |
| `apps/web/src/app/admin/settings/provider-constants.ts` | CREATE | Shared frontend provider display names + options |
| `apps/api-gateway/src/app/common/provider-keys.ts` | CREATE | Shared backend KNOWN_PROVIDER_KEYS constant |
| `apps/api-gateway/src/app/workflows/llm-models.service.ts` | MODIFY | AC 9: providerKey validation on create |
| `apps/api-gateway/src/app/workflows/llm-models.service.spec.ts` | MODIFY | Add [3.1-4-UNIT-064] providerKey validation test |
| `libs/db-layer/src/lib/rls-setup.service.spec.ts` | MODIFY | Add seeding tests [3.1-4-UNIT-065–069] |
| `.env.example` | MODIFY | Add SETTINGS_ENCRYPTION_KEY |

## Dependencies

- **Requires Story 3.1-1** (Settings page must exist)
- **Consumed by Epic 4** (execution engine uses `getDecryptedCredentials()` to authenticate with LLM providers)

## Deferred Items (Tracked)

These items from the original 3.1-1 story are explicitly deferred:

| Item | Deferred To | Rationale |
|------|-------------|-----------|
| **Test Connection** (original AC 7) | Epic 4 | Requires actual LLM provider SDK integration which is built in Epic 4. Add a "Test Connection" button to provider config UI after Epic 4 delivers the provider interface. |
| **Audit Trail** (original AC 8) | Epic 7 (Story 7-2: Audit Logging) | Cross-cutting concern that should apply to ALL admin actions, not just settings. Story 7-2 already covers audit logging. |
| **E2E Tests** (original Task 6) | E2E stories (1E/2E/3E) | E2E test framework doesn't exist yet. Covered by dedicated E2E stories. |

## Definition of Done

- [x] All acceptance criteria met
- [x] Credentials encrypted at rest (verified by test)
- [x] GET responses never contain plaintext credentials
- [x] All tests pass
- [x] Lint passes
- [x] Code review passed

## Change Log

| Date | Author | Changes |
|------|--------|---------|
| 2026-02-05 | Quality Review | Created from 3.1-1 breakdown. New backend entity + encryption + frontend UI for provider credentials. |
| 2026-02-06 | Pre-Dev Review | Added: OpenAI provider support. Updated model seeding (remove deprecated Gemini 2.0, add 2.5 + 3.0 + OpenAI). Changed AC 8 from crash-on-missing-key to graceful degradation. Added AC 9 (providerKey validation). Dedicated "Providers" tab. Renamed frontend service to llm-provider-config.service.ts. Added Task 8 (model seeding update). |
| 2026-02-07 | Implementation | All 8 tasks complete. 63 new tests (16 crypto, 21 service, 3 controller, 12 list, 11 dialog). 873 total tests passing, 0 lint errors across all 4 projects. Status → review. |
| 2026-02-07 | Code Review Fixes | Fixed 8 review findings (3H, 3M, 2L): H1 — AC 9 providerKey validation added to LlmModelsService. H2 — Provider config seeding added to RlsSetupService (boot order: configs before models). H3 — Model seeding updated to 12 models (added vertex/gemini-3.0, openai/gpt-5.2). M1 — Fixed mixed CJS/ESM import in crypto.util.ts. M2 — AC-to-Test traceability table added. M3 — Credential value type validation. L1 — Fixed test ID conflict. L2 — Consolidated duplicated provider constants. 7 new tests (UNIT-064–070), total 70 story tests. |
