<workflow>
  <critical>The workflow execution engine is governed by: {project-root}/_bmad/core/tasks/workflow.xml</critical>
  <critical>You MUST have already loaded and processed: {installed_path}/workflow.yaml</critical>
  <critical>Communicate all responses in {communication_language} and language MUST be tailored to {user_skill_level}</critical>
  <critical>Generate all documents in {document_output_language}</critical>

  <critical>MANDATORY 3-PASS CODE REVIEW:
    Pass 1 ‚Äî Amelia (Dev Agent): Self-review in the same session. Catches obvious gaps, updates story file, verifies her own work.
    Pass 2 ‚Äî Naz (Reviewer Agent): MUST be launched VIA PARTY MODE (invoke party-mode skill). FRESH CONTEXT with a DIFFERENT LLM than Pass 1. Adversarial review. Trusts nothing from Pass 1. NEVER launch as a Task agent ‚Äî ALWAYS party mode.
    Pass 3 ‚Äî Murat (Test Architect): MUST be launched VIA PARTY MODE (invoke party-mode skill). FRESH CONTEXT with a DIFFERENT LLM than Pass 1. Focuses on test quality, coverage, architecture patterns, and risk. NEVER launch as a Task agent ‚Äî ALWAYS party mode.
    ALL THREE PASSES ARE MANDATORY. A story is NOT done until all 3 passes complete and all HIGH/MEDIUM findings are fixed.
    Pass 2 and Pass 3 are ALWAYS executed via party mode ‚Äî the reviewer presents findings directly to the team. The dev agent (Amelia) is the REVIEWEE, not the presenter. She has NO role in presenting findings.
    If you are running Pass 2 or Pass 3 and you are the same agent/session that implemented the code, STOP IMMEDIATELY and inform the user.</critical>

  <critical>üî• YOU ARE AN ADVERSARIAL CODE REVIEWER - Find what's wrong or missing! üî•</critical>
  <critical>Your purpose: Validate story file claims against actual implementation</critical>
  <critical>Challenge everything: Are tasks marked [x] actually done? Are ACs really implemented?</critical>
  <critical>Find 3-10 specific issues in every review minimum - no lazy "looks good" reviews - YOU are so much better than the dev agent
    that wrote this slop</critical>
  <critical>Read EVERY file in the File List - verify implementation against story requirements</critical>
  <critical>Tasks marked complete but not done = CRITICAL finding</critical>
  <critical>Acceptance Criteria not implemented = HIGH severity finding</critical>
  <critical>NEVER downgrade severity. A finding is what it is. NEVER use the word "MVP" to justify, excuse, or defer anything.</critical>
  <critical>Check KNOWN VIOLATION PATTERNS first: (1) tenantId missing in WHERE clauses (Rule 2c), (2) Story file not updated, (3) Undocumented deferrals, (4) N+1 query patterns. These have historically been missed ‚Äî they are priority #1.</critical>
  <critical>Do not review files that are not part of the application's source code. Always exclude the _bmad/ and _bmad-output/ folders from the review. Always exclude IDE and CLI configuration folders like .cursor/ and .windsurf/ and .claude/</critical>

  <critical>PASS 1 VERIFIABLE CHECKLIST REQUIREMENT:
    Pass 1 (Amelia's self-review) must produce a VERIFIABLE CHECKLIST, not prose claims.
    For Rule 2c specifically: list EVERY findOne, find, update, delete, and manager.query() call in changed files.
    For each call, state: (a) file:line, (b) entity type, (c) whether tenantId is in WHERE, (d) if not, why (documented exception or bug).
    This list is what Pass 2 verifies against. If Pass 1 listed 3 calls and Pass 2 finds 8, the discrepancy is a CRITICAL finding.</critical>

  <critical>DEV AGENT SILENCE DURING PASS 2 AND PASS 3:
    During Pass 2 (Naz) and Pass 3 (Murat) party mode presentations, the dev agent (Amelia) is NOT ALLOWED TO:
    - Comment on, defend, explain, or contextualize any finding
    - Attach verdicts (FIX/TRACK/REJECT) to findings she did not generate
    - Summarize or re-present findings that were already presented by the reviewer
    The reviewer presents findings directly to the team. The user decides each finding's fate.
    Amelia may speak ONLY when directly addressed by the user, and ONLY to say "Understood. I will fix [list]."</critical>

  <critical>TRACKED ITEM ENFORCEMENT:
    Every finding marked TRACKED must include a specific story ID in sprint-status.yaml.
    "Track to future story" or "track to 4-test-gaps" without an immediate file write to sprint-status.yaml is NOT tracking.
    The write must happen in the SAME turn as the tracking decision. No exceptions.</critical>

  <critical>MANDATORY FINDING FORMAT: Every finding must use this exact structure. Free-form prose verdicts are NOT acceptable.

    Finding [N]: [one-line description]
      Severity: CRITICAL | HIGH | MEDIUM | LOW
      File:Line: [exact reference, e.g., src/foo/bar.service.ts:142]
      Description: [specific description of the problem and why it matters]
      Resolution: FIX | DEFERRED_TO: [specific-story-id]

    Rules:
    - Resolution must be exactly "FIX" or "DEFERRED_TO: [specific-story-id]"
    - "DEFERRED_TO" with no story-id = INVALID ‚Äî assign a story-id or create a new backlog entry FIRST
    - ALL CRITICAL and HIGH findings: Resolution must be FIX. No exceptions. No deferrals.
    - MEDIUM findings: FIX by default. DEFERRED_TO only with explicit user approval in the same session.
    - LOW findings: FIX or DEFERRED_TO both valid.
    - BANNED resolutions: "acceptable," "livable," "acknowledged," "fine for now," "future cleanup," "out of scope," or any synonym.
      These do not exist. Every finding is FIX or DEFERRED_TO a named story. Period.</critical>

  <!-- Context Management Checkpoint (Rule 41) -->
  <step n="0" goal="Pre-flight context check">
    <critical>NEVER allow context compaction DURING review. Compact BEFORE starting if needed.</critical>
    <action>Check current token usage in this session</action>
    <check if="token_usage > 150000">
      <warning>Context usage high ({token_usage} tokens). Compacting now BEFORE starting review to prevent mid-process data loss.</warning>
      <action>Trigger context compaction</action>
      <action>Wait for compaction to complete</action>
      <check if="token_usage > 150000 after compaction">
        <error>Still above 150k after compaction ({token_usage} tokens). Session exhausted. User should start fresh session for this review to avoid mid-review compaction that loses critical findings.</error>
        <action>Ask user: "Context still above 150k after compaction. Session exhausted. Recommend starting fresh session for clean review. Proceed anyway or start fresh?"</action>
      </check>
    </check>
    <action>Proceed to story loading</action>
  </step>

  <step n="1" goal="Load story and discover changes">
    <action>Use provided {{story_path}} or ask user which story file to review</action>
    <action>Read COMPLETE story file</action>
    <action>Set {{story_key}} = extracted key from filename (e.g., "1-2-user-authentication.md" ‚Üí "1-2-user-authentication") or story
      metadata</action>
    <action>Parse sections: Story, Acceptance Criteria, Tasks/Subtasks, Dev Agent Record ‚Üí File List, Change Log</action>

    <!-- Discover actual changes via git -->
    <action>Check if git repository detected in current directory</action>
    <check if="git repository exists">
      <action>Run `git status --porcelain` to find uncommitted changes</action>
      <action>Run `git diff --name-only` to see modified files</action>
      <action>Run `git diff --cached --name-only` to see staged files</action>
      <action>Compile list of actually changed files from git output</action>
    </check>

    <!-- Cross-reference story File List vs git reality -->
    <action>Compare story's Dev Agent Record ‚Üí File List with actual git changes</action>
    <action>Note discrepancies:
      - Files in git but not in story File List
      - Files in story File List but no git changes
      - Missing documentation of what was actually changed
    </action>

    <invoke-protocol name="discover_inputs" />
    <action>Load {project_context} for coding standards (if exists)</action>
  </step>

  <step n="2" goal="Build review attack plan">
    <action>Extract ALL Acceptance Criteria from story</action>
    <action>Extract ALL Tasks/Subtasks with completion status ([x] vs [ ])</action>
    <action>From Dev Agent Record ‚Üí File List, compile list of claimed changes</action>

    <action>Create review plan:
      1. **AC Validation**: Verify each AC is actually implemented
      2. **Task Audit**: Verify each [x] task is really done
      3. **Code Quality**: Security, performance, maintainability
      4. **Test Quality**: Real tests vs placeholder bullshit
    </action>
  </step>

  <step n="3" goal="Execute adversarial review">
    <critical>VALIDATE EVERY CLAIM - Check git reality vs story claims</critical>

    <!-- Git vs Story Discrepancies -->
    <action>Review git vs story File List discrepancies:
      1. **Files changed but not in story File List** ‚Üí MEDIUM finding (incomplete documentation)
      2. **Story lists files but no git changes** ‚Üí HIGH finding (false claims)
      3. **Uncommitted changes not documented** ‚Üí MEDIUM finding (transparency issue)
    </action>

    <!-- Use combined file list: story File List + git discovered files -->
    <action>Create comprehensive review file list from story File List and git changes</action>

    <!-- AC Validation -->
    <action>For EACH Acceptance Criterion:
      1. Read the AC requirement
      2. Search implementation files for evidence
      3. Determine: IMPLEMENTED, PARTIAL, or MISSING
      4. If MISSING/PARTIAL ‚Üí HIGH SEVERITY finding
    </action>

    <!-- Task Completion Audit -->
    <action>For EACH task marked [x]:
      1. Read the task description
      2. Search files for evidence it was actually done
      3. **CRITICAL**: If marked [x] but NOT DONE ‚Üí CRITICAL finding
      4. Record specific proof (file:line)
    </action>

    <!-- Code Quality Deep Dive -->
    <action>For EACH file in comprehensive review list:
      1. **Security**: Look for injection risks, missing validation, auth issues
      2. **Performance**: N+1 queries, inefficient loops, missing caching
      3. **Error Handling**: Missing try/catch, poor error messages
      4. **Code Quality**: Complex functions, magic numbers, poor naming
      5. **Test Quality**: Are tests real assertions or placeholders?
    </action>

    <check if="total_issues_found lt 3">
      <critical>NOT LOOKING HARD ENOUGH - Find more problems!</critical>
      <action>Re-examine code for:
        - Edge cases and null handling
        - Architecture violations
        - Documentation gaps
        - Integration issues
        - Dependency problems
        - Git commit message quality (if applicable)
      </action>
      <action>Find at least 3 more specific, actionable issues</action>
    </check>
  </step>

  <step n="4" goal="Present findings and fix them">
    <action>Categorize findings: HIGH (must fix), MEDIUM (should fix), LOW (nice to fix)</action>
    <action>Set {{fixed_count}} = 0</action>
    <action>Set {{action_count}} = 0</action>

    <output>**üî• CODE REVIEW FINDINGS, {user_name}!**

      **Story:** {{story_file}}
      **Git vs Story Discrepancies:** {{git_discrepancy_count}} found
      **Issues Found:** {{high_count}} High, {{medium_count}} Medium, {{low_count}} Low

      ## üî¥ CRITICAL ISSUES
      - Tasks marked [x] but not actually implemented
      - Acceptance Criteria not implemented
      - Story claims files changed but no git evidence
      - Security vulnerabilities

      ## üü° MEDIUM ISSUES
      - Files changed but not documented in story File List
      - Uncommitted changes not tracked
      - Performance problems
      - Poor test coverage/quality
      - Code maintainability issues

      ## üü¢ LOW ISSUES
      - Code style improvements
      - Documentation gaps
      - Git commit message quality
    </output>

    <ask>What should I do with these issues?

      1. **Fix them automatically** - I'll update the code and tests
      2. **Create action items** - Add to story Tasks/Subtasks for later
      3. **Show me details** - Deep dive into specific issues

      Choose [1], [2], or specify which issue to examine:</ask>

    <check if="user chooses 1">
      <action>Fix all HIGH and MEDIUM issues in the code</action>
      <action>Add/update tests as needed</action>
      <action>Update File List in story if files changed</action>
      <action>Update story Dev Agent Record with fixes applied</action>
      <action>Set {{fixed_count}} = number of HIGH and MEDIUM issues fixed</action>
      <action>Set {{action_count}} = 0</action>
    </check>

    <check if="user chooses 2">
      <action>Add "Review Follow-ups (AI)" subsection to Tasks/Subtasks</action>
      <action>For each issue: `- [ ] [AI-Review][Severity] Description [file:line]`</action>
      <action>Set {{action_count}} = number of action items created</action>
      <action>Set {{fixed_count}} = 0</action>
    </check>

    <check if="user chooses 3">
      <action>Show detailed explanation with code examples</action>
      <action>Return to fix decision</action>
    </check>
  </step>

  <step n="5" goal="Update story status and sync sprint tracking">
    <!-- Determine new status based on review outcome -->
    <check if="all HIGH and MEDIUM issues fixed AND all ACs implemented">
      <action>Set {{new_status}} = "done"</action>
      <action>Update story Status field to "done"</action>
    </check>
    <check if="HIGH or MEDIUM issues remain OR ACs not fully implemented">
      <action>Set {{new_status}} = "in-progress"</action>
      <action>Update story Status field to "in-progress"</action>
    </check>
    <action>Save story file</action>

    <!-- Determine sprint tracking status -->
    <check if="{sprint_status} file exists">
      <action>Set {{current_sprint_status}} = "enabled"</action>
    </check>
    <check if="{sprint_status} file does NOT exist">
      <action>Set {{current_sprint_status}} = "no-sprint-tracking"</action>
    </check>

    <!-- Sync sprint-status.yaml when story status changes (only if sprint tracking enabled) -->
    <check if="{{current_sprint_status}} != 'no-sprint-tracking'">
      <action>Load the FULL file: {sprint_status}</action>
      <action>Find development_status key matching {{story_key}}</action>

      <check if="{{new_status}} == 'done'">
        <action>Update development_status[{{story_key}}] = "done"</action>
        <action>Save file, preserving ALL comments and structure</action>
        <output>‚úÖ Sprint status synced: {{story_key}} ‚Üí done</output>
      </check>

      <check if="{{new_status}} == 'in-progress'">
        <action>Update development_status[{{story_key}}] = "in-progress"</action>
        <action>Save file, preserving ALL comments and structure</action>
        <output>üîÑ Sprint status synced: {{story_key}} ‚Üí in-progress</output>
      </check>

      <check if="story key not found in sprint status">
        <output>‚ö†Ô∏è Story file updated, but sprint-status sync failed: {{story_key}} not found in sprint-status.yaml</output>
      </check>
    </check>

    <check if="{{current_sprint_status}} == 'no-sprint-tracking'">
      <output>‚ÑπÔ∏è Story status updated (no sprint tracking configured)</output>
    </check>

    <output>**‚úÖ Review Complete!**

      **Story Status:** {{new_status}}
      **Issues Fixed:** {{fixed_count}}
      **Action Items Created:** {{action_count}}

      {{#if new_status == "done"}}Code review complete!{{else}}Address the action items and continue development.{{/if}}
    </output>
  </step>

</workflow>